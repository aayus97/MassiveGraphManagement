{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='./figs/cs-logo.png' width=200></center>\n",
    "\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "<h1>\n",
    "<center>Introduction to Spark </center>\n",
    "</h1>\n",
    "<font  size=\"3\" color='#91053d'>\n",
    "<center >\n",
    "nacera.seghouani@centralesupelec.fr \n",
    "</center>\n",
    "</font>   \n",
    "<hr style=\" border:none; height:3px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Introduction (PLEASE READ ME)\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The purpose of this lab is to acquire  basic **Spark** programming skills that are necessary to develop simple, yet powerful, applications to be executed in a distributed environment.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Spark application runs as a set of independent processes (called the _Executors_) across the machines (a.k.a., _Worker_ nodes) of a cluster, on a distributed data, coordinated by the _Driver_, the process that runs the $main()$ function of the application.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The _Driver_ creates an object called _SparkContext_ that communicates with the underlying cluster manager and coordinates the distribution of the computation across the _Executors_.\n",
    "</font>\n",
    "</p>\n",
    "<center\n",
    "<img src=\"./figs/spark-execution.png\" width=400>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "To install **Apache Spark **, first Download and Install on your machine (<a href=\"https://spark.apache.org/downloads.html\">see this link</a>).  To use python language, you need **pyspark** package (<a href=\"https://spark.apache.org/docs/latest/api/python/getting_started/install.html\">see this link</a>).\n",
    "</p>\n",
    "    <p align=\"justify\">\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font  size=\"3\" color='#91053d'>**Execute the following cell in order to initialize the _SparkContext_.**</font>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization successful\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "sc = pyspark.SparkContext(appName=\"lab1\")\n",
    "print(\"Initialization successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Resilient Distributed Dataset (RDD)\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Spark distributes the data and the computations across the machines of a cluster by using **Resilient Distributed Dataset (RDD)**. \n",
    "An RDD is an immutable distributed collection of data. \n",
    "Each element of an RDD can be an instance of any Python type, including a user-defined class.\n",
    "The _SparkContext_  splits an RDD into multiple _partitions_ and scatters them across different machines of the cluster. \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The distribution of the partitions of an RDD is completely transparent to the application.\n",
    "The only thing a Spark application has to do is to create some RDDs and \n",
    "specify the computations on these RDDs, by using special functions that Spark provides for this purpose. \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "## 2.1 Creating RDDs\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Spark provides two ways to create an RDD:\n",
    "\n",
    "<ul>\n",
    "<li> By distributing a collection of objects.\n",
    "<li> By loading an external dataset (either in a file or a database).\n",
    "</ul>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font size=\"3\" color='#91053d'>**Execute the following code to create an RDD called $words$, where each element is a string taken from the list $wordList$.**</font>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "wordList = [\"Al\", \"Ani\", \"Jackie\", \"Lalitha\", \"Mark\", \"Neil\", \"Nick\", \"Shirin\"]\n",
    "words = sc.parallelize(wordList)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Once created, Spark provides two types of  operations on a RDD:\n",
    "<ol>\n",
    "<li> **Transformations**. A transformation takes in one or more RDDs and returns a new RDD.\n",
    "<li> **Actions**. An action takes in an RDD and outputs a value.\n",
    "</ol>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "One common transformation is filtering data that matches a predicate by using the function $filter$.\n",
    "The function $filter$ is applied on an RDD and takes in a predicate.\n",
    "It loops through each element of the RDD and verifies whether that element satisfies the predicate. The function $filter$ outputs a new RDD whose elements are those that satisfy the predicate.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font size=\"3\" color='#91053d'>**Execute the following code to create an RDD called $nNames$ by retaining only the names whose first letter is 'N' from the RDD $words$ .**</font>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "As mentioned before, the function filter takes in a predicate that is itself a function (returning a boolean value).\n",
    "\n",
    "In the code below, the argument of the function filter is a function that takes in a variable called \n",
    "\"name\"; the type of this variable must match the type of the elements of the RDD words, in this case \n",
    "a string.\n",
    "Then the function returns whether the first character of the string \"name\" is N. \n",
    "\n",
    "'''\n",
    "nNames = words.filter(lambda name: name[:1]=='N') \n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The transformation $filter$ above is not executed by Spark until an action is called on the RDD $nNames$.\n",
    "Spark executes  a transformation on an RDD  when an action is invoked on the RDD itself. This is called _lazy evaluation_. \n",
    "    </font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "One example of action is the function $first()$ that returns the first element in the RDD.\n",
    "    </font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font size=\"3\" color='#91053d'>**Execute the following code to print the first element of the RDD $nNames$.**</font>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neil\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "REMEMBER: the variable nNames has been defined in the cell above, so \n",
    "it is VISIBLE in this cell as well as in the cells below the current one\n",
    "'''\n",
    "print(nNames.first())\n",
    "\n",
    "# print(nNames.collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The previous example  shows that a Spark application is  a sequence of operations that create, transform and perform  actions on RDDs.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The following code shows an example of creation of RDD from an external dataset, a Neo4j database log text file. \n",
    "The function $textFile()$ takes in the path to the input text file and returns an RDD, where each element is a line of the file.\n",
    "The code goes through the following steps:\n",
    "\n",
    "<ol>\n",
    "<li> **RDD creation**. Creates an RDD called $lines$, where each element is a line from the input text file.\n",
    "<li> **RDD filter**. Creates an RDD called $exceptions$ from the RDD $lines$ by only retaining the lines containing the string \"exception\".\n",
    "<li> **Action count()**. Counts the number of elements of the RDD $exceptions$. \n",
    "<li> **Print first line**. Prints the first line of the RDD $exceptions$. \n",
    "</ol>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Here we also see  the _lazy evaluation_ of transformations. \n",
    "Spark does not execute the function $textFile()$ as soon as it is invoked.\n",
    "Instead, it waits until the first action $count()$ is invoked.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font size=\"3\" color='#91053d'>**Execute the following code.**</font>\n",
    " <hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of exception lines  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2017-10-06 13:12:26.097+0000 ERROR Failed to start Neo4j: Starting Neo4j failed: Component \\'org.neo4j.server.database.LifecycleManagingDatabase@2814f71a\\' was successfully initialized, but failed to start. Please see the attached cause exception \"Format version is not supported (resource BufferedChecksumIndexInput(MMapIndexInput(path=\"/Users/quercini_gia/Documents/software/neo4j-community-3.2.5/data/databases/social-network/upgrade/index/lucene/relationship/crosslinks/segments_1\"))): -11 (needs to be between 1071082519 and 1071082519). This version of Lucene only supports indexes created with release 4.0 and later.\". Starting Neo4j failed: Component \\'org.neo4j.server.database.LifecycleManagingDatabase@2814f71a\\' was successfully initialized, but failed to start. Please see the attached cause exception \"Format version is not supported (resource BufferedChecksumIndexInput(MMapIndexInput(path=\"/Users/quercini_gia/Documents/software/neo4j-community-3.2.5/data/databases/social-network/upgrade/index/lucene/relationship/crosslinks/segments_1\"))): -11 (needs to be between 1071082519 and 1071082519). This version of Lucene only supports indexes created with release 4.0 and later.\".']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = \"./data/neo4j.log\"\n",
    "\n",
    "# 1. RDD creation\n",
    "lines = sc.textFile(data_file)\n",
    "\n",
    "#2. RDD filter\n",
    "exceptions = lines.filter(lambda line : \"exception\" in line)\n",
    "\n",
    "#3. Action count()\n",
    "nbLines = exceptions.count()\n",
    "print(\"Number of exception lines \", nbLines)\n",
    "\n",
    "#4. Print first line.\n",
    "exceptions.take(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 2.2 Transformations\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Here are some common transformations in Spark. \n",
    "In the following list, $r$ denotes the RDD on which the transformation is invoked.\n",
    "<ol>\n",
    "<li> $r.filter()$. Returns an RDD consisting of only the elements of the input RDD $r$ that satisfy a predicate.\n",
    "<li> $r.map(func)$. Applies a function $func$ to each element of the input RDD $r$ and returns an RDD of the result.\n",
    "<li> $r.flatMap(func)$. Same as $map()$, but used when $map()$ would return an RDD where each element is a list.\n",
    "<li> $r.union(other)$. Takes in two RDDs ($r$ and $other$) and returns an RDD that contains the elements from both. Unlike the mathematical operation, $union$ in Spark does not remove the duplicates.\n",
    "<li> $r.intersection(other)$. Takes in two RDDs ($r$ and $other$) and returns an RDD that contains the elements found in both.\n",
    "<li> $r.subtract(other)$. Takes in two RDDs ($r$ and $other$) and returns an RDD that contains the elements from the RDD $r$, except those that are found in $other$.\n",
    "<li> $r.cartesian(other)$. Takes in two RDDs ($r$ and $other$) and returns an RDD that contains the Cartesian product of both.\n",
    "<li> $r.distinct()$. Returns an RDD that contains the same elements as the input RDD $r$ without duplicates.\n",
    "</ol>\n",
    "    \n",
    "We are now going to look at an example of use of these transformations.\n",
    "</font>    \n",
    "</p>\n",
    "\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font size=\"3\" color='#91053d'>**Execute the following code to create the two RDDs $r1$ and $r2$**</font>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "r1 = sc.parallelize([1, 2, 3, 4])\n",
    "r2 = sc.parallelize([3, 4, 5, 6, 7])\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of $map()$\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Here we see an example of use of the transformation $map()$.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "<font size=\"3\" color='#91053d'>**Execute the following code to create:\n",
    " <ol>\n",
    " <li> an RDD $square$, where each element is the square of the corresponding element in $r1$; \n",
    " <li> an RDD $half$, where each element is the half of the corresponding element in $r2$.**\n",
    " </ol>\n",
    " </font>\n",
    " <hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of RDD square  [1, 4, 9, 16]\n",
      "Elements of the RDD half  [1.5, 2.0, 2.5, 3.0, 3.5]\n"
     ]
    }
   ],
   "source": [
    "square = r1.map(lambda x : x*x)\n",
    "half = r2.map(lambda x: x/2)\n",
    "\n",
    "# The function collect() is an action that transforms the RDD into a Python list that can be printed.\n",
    "print(\"Elements of RDD square \", square.collect())\n",
    "print(\"Elements of the RDD half \", half.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of $flatMap()$\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The transformation $flatMap()$ works by applying the transformation $map()$ on the input RDD; \n",
    "if each element of the resulting RDD is a list, $flatMap()$ returns an RDD where all lists are merged.\n",
    "In other words, when $map()$ returns an RDD where the elements are lists, $flatMap()$ returns an RDD where the elements are the values of the list.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Let's see an example. Suppose that we want to return an RDD from $r1$ where each element is associated to its square. \n",
    "More precisely, the output RDD will be as follows:\n",
    "<center>\n",
    "[ [1, 1], [2, 4], [3, 9], [4, 16] ]\n",
    "</center>\n",
    "</font>    \n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font size=\"3\" color='#91053d'>**Execute the following code to create an RDD $squares$ where each element of $r1$ is associated to its square.**</font>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of RDD squares  [[1, 1], [2, 4], [3, 9], [4, 16]]\n"
     ]
    }
   ],
   "source": [
    "squares = r1.map(lambda x : [x, x*x])\n",
    "print(\"Elements of RDD squares \", squares.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "\n",
    "As you can see, each element of the RDD $squares$ is a list of two elements; indeed, \n",
    "after calling the action $collect()$, we obtain a list of lists in Python. \n",
    "In order to obtain a simple list of elements, we flatten the lists with $flatMap()$.\n",
    "More precisely, with $flatMap()$ we obtain the following RDD:\n",
    "<center>\n",
    "[1, 1, 2, 4, 3, 9, 4, 16]\n",
    "</center>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    " <font size=\"3\" color='#91053d'>**Execute the following code to create an RDD $squares$ where each element of $r1$ is associated to its square (but each element is just a value instead of a list of values)**</font>\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of RDD squares  [1, 1, 2, 4, 3, 9, 4, 16]\n"
     ]
    }
   ],
   "source": [
    "squares = r1.flatMap(lambda x : [x, x*x])\n",
    "print(\"Elements of RDD squares \", squares.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of set transformations\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The set transformations are $union$, $intersection$, $subtract$ and $cartesian$.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font size=\"3\" color='#91053d'>**Execute the following code to see an example of these transformations**</font>\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of RDD r1  [1, 2, 3, 4]\n",
      "Elements of RDD r2  [3, 4, 5, 6, 7]\n",
      "Elements of RDD union  [1, 2, 3, 4, 3, 4, 5, 6, 7]\n",
      "Elements of RDD intesection  [4, 3]\n",
      "Elements of RDD subtract  [1, 2]\n",
      "Elements of RDD cartesian  [(1, 3), (1, 4), (2, 3), (2, 4), (1, 5), (1, 6), (2, 5), (2, 6), (1, 7), (2, 7), (3, 3), (3, 4), (4, 3), (4, 4), (3, 5), (3, 6), (4, 5), (4, 6), (3, 7), (4, 7)]\n"
     ]
    }
   ],
   "source": [
    "union = r1.union(r2)\n",
    "intersection = r1.intersection(r2)\n",
    "subtract = r1.subtract(r2)\n",
    "cartesian = r1.cartesian(r2)\n",
    "\n",
    "print(\"Elements of RDD r1 \", r1.collect())\n",
    "print(\"Elements of RDD r2 \", r2.collect())\n",
    "print(\"Elements of RDD union \", union.collect())\n",
    "print(\"Elements of RDD intesection \", intersection.collect())\n",
    "print(\"Elements of RDD subtract \", subtract.collect())\n",
    "print(\"Elements of RDD cartesian \", cartesian.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of $distinct()$\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The transformation $distinct()$ returns an RDD that contains the same elements as the input RDD without the duplicates.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    " <font size=\"3\" color='#91053d'>**Execute the following code to see an example of use of $distinct()$**</font>\n",
    " <hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of the RDD union:  [1, 2, 3, 4, 3, 4, 5, 6, 7]\n",
      "Elements of the RDD union (with no duplicates):  [4, 1, 5, 2, 6, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "nodup = union.distinct()\n",
    "print(\"Elements of the RDD union: \", union.collect())\n",
    "print(\"Elements of the RDD union (with no duplicates): \", nodup.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 2.3. Actions\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Here are some common actions in Spark. \n",
    "As with transformations, $r$ denotes the RDD on which the action is invoked.\n",
    "\n",
    "<ol>\n",
    "<li> $r.reduce(func)$. Performs a pair-wise application of the given function $func$ to the elements of the input RDD $r$.\n",
    "<li> $r.collect()$. Returns a list with all the elements of the input RDD $r$.\n",
    "<li> $r.count()$. Returns the number of elements in the input RDD $r$.\n",
    "<li> $r.countByValue()$. Returns the number of times each element occurs in the input RDD $r$.\n",
    "<li> $r.take(num)$. Prints the first $num$ elements of the input RDD $r$.\n",
    "<li> $r.top(num)$. Prints the top $num$ elements of the input RDD $r$ (sorted in decreasing order).\n",
    "</ol>\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of $reduce(func)$\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The action $reduce(func)$ performs a pair-wise application of the given function $func$ on the elements of the input RDD. \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "<font size=\"3\" color='#91053d'>**Execute the following code to sum all values of the RDD $r1$**</font>\n",
    "<hr style=\" border:none; height:2px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of the RDD r1  [1, 2, 3, 4]\n",
      "Sum of the elements of the RDD r1:  10\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The function passed to the reduce MUST take in two arguments \n",
    "that have the same type as the elements of the input RDD\n",
    "'''\n",
    "sum = r1.reduce(lambda x, y : x + y)\n",
    "print(\"Elements of the RDD r1 \", r1.collect())\n",
    "print(\"Sum of the elements of the RDD r1: \", sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of $countByValue()$\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The action $countByValue()$ counts the number of the occurrences of each element of the input RDD.\n",
    "The result is a Python dictionary, where a key is an element of the input RDD and the corresponding value the number of its occurrences.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    "<font size=\"3\" color='#91053d'>**Execute the following code to get the number of occurrences of each element in the RDD $union$**</font>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of the RDD union  [1, 2, 3, 4, 3, 4, 5, 6, 7]\n",
      "Occurrences of each element in the RDD union:\n",
      "1  -->  1  occurrences\n",
      "2  -->  1  occurrences\n",
      "3  -->  2  occurrences\n",
      "4  -->  2  occurrences\n",
      "5  -->  1  occurrences\n",
      "6  -->  1  occurrences\n",
      "7  -->  1  occurrences\n"
     ]
    }
   ],
   "source": [
    "occurrences = union.countByValue()\n",
    "print(\"Elements of the RDD union \", union.collect())\n",
    "print(\"Occurrences of each element in the RDD union:\")\n",
    "for k, v in occurrences.items():\n",
    "    print(k, \" --> \",  v, \" occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercises \n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Now it's your turn! \n",
    "You're going to apply yourself the transformations and the actions that we've seen so far. \n",
    "To this extent, we will use the file **./data/moby-dick.txt** that contains the text of the famous novel by Herman Melville. \n",
    "The goal of this exercise is to count the number of words in the file and the number of occurrences of each word.\n",
    "</font>\n",
    "</p>\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "First, we are going to preprocess the input text, by removing punctuation and special characters and to lowercase all words.\n",
    "</font>\n",
    "</p>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    "###  Exercise 1\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The function $preprocess()$ defined below takes in an RDD that contains the lines of the input text file and returns an RDD where each element is a word. In the code below, the first step of the function $preprocess()$, which removes the non-letter characters from the input RDD by using a regular expression, is already implemented.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\" color='#91053d'>\n",
    "**Complete the function $proprocess()$ with the following steps:**\n",
    "<ol start=\"2\">\n",
    "<li>    Apply a transformation on the RDD $text$ to obtain an RDD $words$, where each element is a word from the file.\n",
    "<li>    Filter out from the RDD $words$ the words with length 0.\n",
    "<li>    Lowercase all words of the RDD $words$.\n",
    "<li>    Return the RDD $words$.\n",
    "</ol>\n",
    "Execute the code, which will print the number of words in the input text file and the top-100 most frequent words.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words  208240\n",
      "Top-100 most frequent words\n",
      "the ---> 14065\n",
      "of ---> 6440\n",
      "and ---> 6257\n",
      "a ---> 4535\n",
      "to ---> 4491\n",
      "in ---> 4050\n",
      "that ---> 2900\n",
      "his ---> 2485\n",
      "it ---> 2343\n",
      "i ---> 1930\n",
      "but ---> 1763\n",
      "he ---> 1729\n",
      "as ---> 1698\n",
      "with ---> 1690\n",
      "is ---> 1672\n",
      "was ---> 1618\n",
      "for ---> 1574\n",
      "all ---> 1464\n",
      "this ---> 1351\n",
      "at ---> 1295\n",
      "by ---> 1169\n",
      "not ---> 1121\n",
      "from ---> 1071\n",
      "him ---> 1035\n",
      "so ---> 1030\n",
      "on ---> 1028\n",
      "be ---> 1024\n",
      "whale ---> 891\n",
      "one ---> 875\n",
      "you ---> 851\n",
      "had ---> 765\n",
      "now ---> 759\n",
      "there ---> 755\n",
      "have ---> 751\n",
      "or ---> 686\n",
      "were ---> 674\n",
      "they ---> 639\n",
      "which ---> 617\n",
      "then ---> 614\n",
      "me ---> 606\n",
      "some ---> 604\n",
      "their ---> 603\n",
      "when ---> 591\n",
      "are ---> 584\n",
      "my ---> 584\n",
      "an ---> 583\n",
      "like ---> 567\n",
      "no ---> 566\n",
      "upon ---> 560\n",
      "what ---> 537\n",
      "into ---> 517\n",
      "out ---> 509\n",
      "more ---> 499\n",
      "up ---> 496\n",
      "if ---> 469\n",
      "its ---> 457\n",
      "them ---> 450\n",
      "old ---> 436\n",
      "man ---> 433\n",
      "we ---> 426\n",
      "would ---> 421\n",
      "ahab ---> 417\n",
      "ye ---> 416\n",
      "been ---> 408\n",
      "over ---> 397\n",
      "other ---> 393\n",
      "these ---> 387\n",
      "ship ---> 378\n",
      "will ---> 377\n",
      "only ---> 369\n",
      "such ---> 364\n",
      "whales ---> 363\n",
      "sea ---> 363\n",
      "though ---> 362\n",
      "down ---> 358\n",
      "yet ---> 339\n",
      "who ---> 330\n",
      "her ---> 321\n",
      "time ---> 321\n",
      "any ---> 320\n",
      "very ---> 317\n",
      "long ---> 313\n",
      "still ---> 307\n",
      "about ---> 305\n",
      "those ---> 302\n",
      "than ---> 300\n",
      "do ---> 297\n",
      "captain ---> 292\n",
      "before ---> 291\n",
      "great ---> 289\n",
      "said ---> 288\n",
      "has ---> 287\n",
      "here ---> 284\n",
      "seemed ---> 280\n",
      "must ---> 279\n",
      "two ---> 278\n",
      "last ---> 273\n",
      "most ---> 273\n",
      "head ---> 263\n",
      "thou ---> 262\n"
     ]
    }
   ],
   "source": [
    "import operator;\n",
    "import re;\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    '''\n",
    "    Regular expression for removing all non-letter characters in the file.\n",
    "    '''\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    '''\n",
    "    Step 1. \n",
    "    Remove the non-letter characters.\n",
    "    After this transformation, mobydick is an RDD where each element is a line of the file\n",
    "    '''\n",
    "    text = text.map(lambda line: regex.sub('', line))\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Step 2. \n",
    "    Split each line into words.\n",
    "    This will create an RDD where each element is a single word.\n",
    "    '''\n",
    "    words = text.flatMap(lambda line: line.split())\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Step 3. \n",
    "    Filter out words with length 0.\n",
    "    This ensures that empty strings are removed from the RDD.\n",
    "    '''\n",
    "    words = words.filter(lambda word: len(word) > 0)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Step 4.\n",
    "    Convert all words to lowercase.\n",
    "    This ensures case insensitivity when counting word occurrences.\n",
    "    '''\n",
    "    words = words.map(lambda word: word.lower())\n",
    "    \n",
    "    '''\n",
    "    Step 5.\n",
    "    Returns the RDD words.\n",
    "    '''\n",
    "    return words\n",
    "\n",
    "\n",
    "# Reads the novel into a RDD\n",
    "mobydick = sc.textFile('./data/moby-dick.txt')\n",
    "\n",
    "words = preprocess(mobydick)\n",
    "\n",
    "print(\"Number of words \", words.count())\n",
    "\n",
    "'''\n",
    "Top-100 most frequent words\n",
    "'''\n",
    "# countByValue() counts the number of occurrences of each word.\n",
    "# It returns a dictionary. \n",
    "# The function sorted sorts by value and returns a list of tuples (w, f), where w is a word \n",
    "# and f its number of occurrences.\n",
    "# The list will be sorted in ascending order of number of occurrences\n",
    "occurrences = sorted(words.countByValue().items(), key=operator.itemgetter(1))\n",
    "\n",
    "# Reverse the order\n",
    "occurrences.reverse()\n",
    "print(\"Top-100 most frequent words\")\n",
    "i = 0\n",
    "for (w, f) in occurrences:\n",
    "    i += 1\n",
    "    if i > 100:\n",
    "        break\n",
    "    print(w,\"--->\",f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In the previous example, you've probably noticed that the top-100 most frequent words are _functions_ words, such as articles, pronouns, conjunctions. These words have little lexical meaning and express a grammatical relationship with the other words. \n",
    "In many applications, these functions words are not useful and they can be ignored.\n",
    "For instance, when you submit a query against a search engine (e.g., \"restaurants in Paris\"), function words (e.g., \"in\") are not useful to retrieve the Web pages relevant to the query.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In text processing applications, function words are part of a list of _stop words_ that include all words that are frequently used and give little semantic contribution to the content of a text.\n",
    "Since there isn't any agreement as to the definition of _stop word_, it's possible to find many such lists on the Web.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\" color='#91053d'>**Write a function $preprocess()$ that applies the same operations as the function that you defined above and, in addition, removes the stop words. This function $preprocess()$ takes in: (i) an RDD that contains the lines of the input text file (as above) and (ii) an RDD containing the stop words. The function returns an RDD with the words of the input text file. **</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "HINT: The file with the list of stop words is **./data/stopwords.txt**. All stop words are already lowercased.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words after stopword removal  102358\n",
      "Top-100 most frequent words\n",
      "whale ,  891\n",
      "one ,  875\n",
      "old ,  436\n",
      "man ,  433\n",
      "ahab ,  417\n",
      "ye ,  416\n",
      "ship ,  378\n",
      "whales ,  363\n",
      "sea ,  363\n",
      "though ,  362\n",
      "time ,  321\n",
      "long ,  313\n",
      "captain ,  292\n",
      "great ,  289\n",
      "said ,  288\n",
      "seemed ,  280\n",
      "must ,  279\n",
      "two ,  278\n",
      "last ,  273\n",
      "head ,  263\n",
      "see ,  257\n",
      "way ,  253\n",
      "white ,  247\n",
      "little ,  247\n",
      "boat ,  241\n",
      "round ,  239\n",
      "three ,  235\n",
      "sperm ,  232\n",
      "first ,  229\n",
      "stubb ,  227\n",
      "men ,  224\n",
      "say ,  223\n",
      "every ,  223\n",
      "us ,  221\n",
      "well ,  221\n",
      "much ,  218\n",
      "queequeg ,  211\n",
      "good ,  195\n",
      "hand ,  194\n",
      "side ,  183\n",
      "go ,  183\n",
      "thing ,  181\n",
      "look ,  178\n",
      "boats ,  175\n",
      "made ,  174\n",
      "away ,  173\n",
      "chapter ,  172\n",
      "come ,  170\n",
      "starbuck ,  169\n",
      "many ,  161\n",
      "deck ,  158\n",
      "water ,  158\n",
      "far ,  157\n",
      "seen ,  156\n",
      "day ,  156\n",
      "eyes ,  153\n",
      "ships ,  152\n",
      "sir ,  151\n",
      "sort ,  151\n",
      "cried ,  149\n",
      "back ,  148\n",
      "world ,  147\n",
      "part ,  147\n",
      "thought ,  147\n",
      "know ,  145\n",
      "whole ,  137\n",
      "oh ,  137\n",
      "right ,  136\n",
      "aye ,  135\n",
      "life ,  134\n",
      "crew ,  133\n",
      "air ,  133\n",
      "thus ,  132\n",
      "thee ,  128\n",
      "night ,  128\n",
      "tell ,  128\n",
      "soon ,  128\n",
      "came ,  126\n",
      "take ,  126\n",
      "things ,  125\n",
      "hands ,  125\n",
      "feet ,  123\n",
      "small ,  122\n",
      "pequod ,  121\n",
      "till ,  119\n",
      "something ,  118\n",
      "line ,  116\n",
      "think ,  116\n",
      "god ,  115\n",
      "thy ,  113\n",
      "found ,  113\n",
      "towards ,  113\n",
      "full ,  111\n",
      "times ,  110\n",
      "along ,  110\n",
      "dont ,  110\n",
      "another ,  110\n",
      "make ,  109\n",
      "nothing ,  109\n",
      "called ,  108\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text, stopwords):\n",
    "    '''\n",
    "    Regular expression for removing all non-letter characters in the file.\n",
    "    '''\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    '''\n",
    "    Step 1. \n",
    "    Remove the non-letter characters.\n",
    "    After this transformation, mobydick is an RDD where each element is a line of the file\n",
    "    '''\n",
    "    text = text.map(lambda line: regex.sub('', line))\n",
    "    \n",
    "    '''\n",
    "    Step 2.\n",
    "    Obtain the RDD containing the words in the file.\n",
    "    Note that the function line.split() returns the list of words in a line. \n",
    "    Therefore, each element of the RDD words would be a list, shouldn't we use flatMap.\n",
    "    '''\n",
    "    words = text.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "    '''\n",
    "    Step 3.\n",
    "    Filter out the words with length 0.\n",
    "    '''\n",
    "    words = words.filter(lambda word: len(word) > 0)\n",
    "    \n",
    "    '''\n",
    "    Step 4.\n",
    "    Lowercase all words of the RDD $words$\n",
    "    '''\n",
    "    words = words.map(lambda word: word.lower())\n",
    "    \n",
    "    '''\n",
    "    Step 5. Remove stop words.\n",
    "    Convert the stopwords RDD to a Python set for efficient lookup, \n",
    "    and filter the words RDD to exclude any word in the stopwords set.\n",
    "    '''\n",
    "    stopwords_set = set(stopwords.collect())  # Convert stopwords RDD to a set\n",
    "    words = words.filter(lambda word: word not in stopwords_set)\n",
    "    \n",
    "    \n",
    "    # Returns the words\n",
    "    return words\n",
    "\n",
    "\n",
    "# Load the stopwords to an RDD\n",
    "stopwords = sc.textFile(\"./data/stopwords.txt\")\n",
    "words = preprocess(mobydick, stopwords)\n",
    "print(\"Number of words after stopword removal \", words.count())\n",
    "\n",
    "# Count the number of occurrences as before.\n",
    "occurrences = sorted(words.countByValue().items(), key=operator.itemgetter(1))\n",
    "# Reverse the order\n",
    "occurrences.reverse()\n",
    "print(\"Top-100 most frequent words\")\n",
    "i = 0\n",
    "for (w, f) in occurrences:\n",
    "    i += 1\n",
    "    if i > 100:\n",
    "        break\n",
    "    print(w, \", \", f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Pair RDDs \n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Pair RDDs are simply RDDs where each element is a key-value pair. They are useful for expressing _MapReduce_ computations. \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We are now going to look at examples of  transformations and actions on pair RDDs.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Creation of Pair RDDs\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "One common way to create a pair RDD is to transform an existing RDD with a $map()$. \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\" color='#91053d'>**Execute the following code to create a Pair RDD $kvwords$ from the RDD $words$. Each element of $kvwords$ is a pair where the key is a word and the value is 1**</font>\n",
    "</p>\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chapter', 1), ('loomings', 1), ('call', 1), ('ishmael', 1), ('years', 1), ('agonever', 1), ('mind', 1), ('long', 1), ('preciselyhaving', 1), ('little', 1), ('money', 1), ('purse', 1), ('nothing', 1), ('particular', 1), ('interest', 1), ('shore', 1), ('thought', 1), ('sail', 1), ('little', 1), ('see', 1), ('watery', 1), ('part', 1), ('world', 1), ('way', 1), ('driving', 1), ('spleen', 1), ('regulating', 1), ('circulation', 1), ('whenever', 1), ('find', 1), ('growing', 1), ('grim', 1), ('mouth', 1), ('whenever', 1), ('damp', 1), ('drizzly', 1), ('november', 1), ('soul', 1), ('whenever', 1), ('find', 1), ('involuntarily', 1), ('pausing', 1), ('coffin', 1), ('warehouses', 1), ('bringing', 1), ('rear', 1), ('every', 1), ('funeral', 1), ('meet', 1), ('especially', 1), ('whenever', 1), ('hypos', 1), ('get', 1), ('upper', 1), ('hand', 1), ('requires', 1), ('strong', 1), ('moral', 1), ('principle', 1), ('prevent', 1), ('deliberately', 1), ('stepping', 1), ('street', 1), ('methodically', 1), ('knocking', 1), ('peoples', 1), ('hats', 1), ('offthen', 1), ('account', 1), ('high', 1), ('time', 1), ('get', 1), ('sea', 1), ('soon', 1), ('substitute', 1), ('pistol', 1), ('ball', 1), ('philosophical', 1), ('flourish', 1), ('cato', 1), ('throws', 1), ('sword', 1), ('quietly', 1), ('take', 1), ('ship', 1), ('nothing', 1), ('surprising', 1), ('knew', 1), ('men', 1), ('degree', 1), ('time', 1), ('cherish', 1), ('nearly', 1), ('feelings', 1), ('towards', 1), ('ocean', 1), ('insular', 1), ('city', 1), ('manhattoes', 1), ('belted', 1)]\n"
     ]
    }
   ],
   "source": [
    "kvwords = words.map(lambda word : (word, 1))\n",
    "print(kvwords.take(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Transformations on Pair RDDs\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "All transformations applied to standard RDDs can be applied to Pair RDDs as well. The only difference is that any function that is passed to a transformation must take in tuples instead of single values.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In addition, the following transformations can **only** be applied to Pair RDDs:\n",
    "<ol>\n",
    "<li> $r.reduceByKey(func)$. It applies the given function $func$ pairwise to all elements of the input RDD $r$ that are associated to the same key. \n",
    "<li> $r.sortBy(func, asc)$. Returns an RDD where the elements of the input RDD $r$ are sorted according to the given criteria.\n",
    "<li> $r.groupByKey()$. Groups values with the same key from the input RDD $r$.\n",
    "<li> $r.keys()$. Returns an RDD where the elements are the keys from the input RDD $r$.\n",
    "<li> $r.values()$. Returns an RDD where the elements are the values from the input RDD $r$.\n",
    "</ol>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "<font size=\"3\" color='#91053d'>**Execute the following code to count the number of occurrences of each word in the RDD $kvwords$ and print the top-100 most frequent words**</font>\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('whale', 891),\n",
       " ('one', 875),\n",
       " ('old', 436),\n",
       " ('man', 433),\n",
       " ('ahab', 417),\n",
       " ('ye', 416),\n",
       " ('ship', 378),\n",
       " ('sea', 363),\n",
       " ('whales', 363),\n",
       " ('though', 362),\n",
       " ('time', 321),\n",
       " ('long', 313),\n",
       " ('captain', 292),\n",
       " ('great', 289),\n",
       " ('said', 288),\n",
       " ('seemed', 280),\n",
       " ('must', 279),\n",
       " ('two', 278),\n",
       " ('last', 273),\n",
       " ('head', 263),\n",
       " ('see', 257),\n",
       " ('way', 253),\n",
       " ('little', 247),\n",
       " ('white', 247),\n",
       " ('boat', 241),\n",
       " ('round', 239),\n",
       " ('three', 235),\n",
       " ('sperm', 232),\n",
       " ('first', 229),\n",
       " ('stubb', 227),\n",
       " ('men', 224),\n",
       " ('say', 223),\n",
       " ('every', 223),\n",
       " ('us', 221),\n",
       " ('well', 221),\n",
       " ('much', 218),\n",
       " ('queequeg', 211),\n",
       " ('good', 195),\n",
       " ('hand', 194),\n",
       " ('go', 183),\n",
       " ('side', 183),\n",
       " ('thing', 181),\n",
       " ('look', 178),\n",
       " ('boats', 175),\n",
       " ('made', 174),\n",
       " ('away', 173),\n",
       " ('chapter', 172),\n",
       " ('come', 170),\n",
       " ('starbuck', 169),\n",
       " ('many', 161),\n",
       " ('water', 158),\n",
       " ('deck', 158),\n",
       " ('far', 157),\n",
       " ('seen', 156),\n",
       " ('day', 156),\n",
       " ('eyes', 153),\n",
       " ('ships', 152),\n",
       " ('sort', 151),\n",
       " ('sir', 151),\n",
       " ('cried', 149),\n",
       " ('back', 148),\n",
       " ('thought', 147),\n",
       " ('world', 147),\n",
       " ('part', 147),\n",
       " ('know', 145),\n",
       " ('oh', 137),\n",
       " ('whole', 137),\n",
       " ('right', 136),\n",
       " ('aye', 135),\n",
       " ('life', 134),\n",
       " ('crew', 133),\n",
       " ('air', 133),\n",
       " ('thus', 132),\n",
       " ('tell', 128),\n",
       " ('night', 128),\n",
       " ('soon', 128),\n",
       " ('thee', 128),\n",
       " ('take', 126),\n",
       " ('came', 126),\n",
       " ('things', 125),\n",
       " ('hands', 125),\n",
       " ('feet', 123),\n",
       " ('small', 122),\n",
       " ('pequod', 121),\n",
       " ('till', 119),\n",
       " ('something', 118),\n",
       " ('think', 116),\n",
       " ('line', 116),\n",
       " ('god', 115),\n",
       " ('towards', 113),\n",
       " ('thy', 113),\n",
       " ('found', 113),\n",
       " ('full', 111),\n",
       " ('another', 110),\n",
       " ('dont', 110),\n",
       " ('along', 110),\n",
       " ('times', 110),\n",
       " ('make', 109),\n",
       " ('nothing', 109),\n",
       " ('whaling', 108)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "kvwords is an RDD where each element is (w, 1), w is a word from the novel Moby Dick.\n",
    "If a word w occurs k times, there will be k pairs (w, 1) in the RDD kvwords.\n",
    "To compute the number of occurrences of each word, \n",
    "we should just sum the 1s associated to all pairs with the same key, for all keys.\n",
    "This is achieved with the reduceByKey() function.\n",
    "'''\n",
    "occurrences = kvwords.reduceByKey(lambda x, y : x + y)\n",
    "\n",
    "'''\n",
    "Each element of the RDD occurrences is a pair (w, f), where w is a word and f is the number of occurrences of w.\n",
    "Finally, we sort the RDD occurrences by value by using sortBy(). \n",
    "The first argument of the function sortBy() is a function that takes in a key-value pair and returns the \n",
    "value (x[1]). \n",
    "The second argument specifies the order (ascending or descending).\n",
    "'''\n",
    "occurrences = occurrences.sortBy(lambda x: x[1], ascending=False)\n",
    "occurrences.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Another way to obtain the same result, would be to:\n",
    "<ol>\n",
    "<li> Apply the transformation $groupByKey()$ to the RDD $kvwords$. The result is a RDD $occurrences$, where each element is a pair $(w, L)$, $w$ is a word and $L$ is a list of 1s (as many as the number of occurrences of $w$).\n",
    "<li> Apply a transformation $map()$ to the RDD $occurrences$ to obtain a new RDD $occurrences$, where each element is a pair $(w, len(L))$, $w$ being a word and $len(L)$ being the number of elements of $L$.\n",
    "<li> Sort the RDD $occurrences$ as before.\n",
    "</ol>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "<font size=\"3\" color='#91053d'>**Execute the following code to see the result with this alternative solution**</font>\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('whale', 891),\n",
       " ('one', 875),\n",
       " ('old', 436),\n",
       " ('man', 433),\n",
       " ('ahab', 417),\n",
       " ('ye', 416),\n",
       " ('ship', 378),\n",
       " ('sea', 363),\n",
       " ('whales', 363),\n",
       " ('though', 362),\n",
       " ('time', 321),\n",
       " ('long', 313),\n",
       " ('captain', 292),\n",
       " ('great', 289),\n",
       " ('said', 288),\n",
       " ('seemed', 280),\n",
       " ('must', 279),\n",
       " ('two', 278),\n",
       " ('last', 273),\n",
       " ('head', 263),\n",
       " ('see', 257),\n",
       " ('way', 253),\n",
       " ('little', 247),\n",
       " ('white', 247),\n",
       " ('boat', 241),\n",
       " ('round', 239),\n",
       " ('three', 235),\n",
       " ('sperm', 232),\n",
       " ('first', 229),\n",
       " ('stubb', 227),\n",
       " ('men', 224),\n",
       " ('say', 223),\n",
       " ('every', 223),\n",
       " ('us', 221),\n",
       " ('well', 221),\n",
       " ('much', 218),\n",
       " ('queequeg', 211),\n",
       " ('good', 195),\n",
       " ('hand', 194),\n",
       " ('go', 183),\n",
       " ('side', 183),\n",
       " ('thing', 181),\n",
       " ('look', 178),\n",
       " ('boats', 175),\n",
       " ('made', 174),\n",
       " ('away', 173),\n",
       " ('chapter', 172),\n",
       " ('come', 170),\n",
       " ('starbuck', 169),\n",
       " ('many', 161),\n",
       " ('water', 158),\n",
       " ('deck', 158),\n",
       " ('far', 157),\n",
       " ('seen', 156),\n",
       " ('day', 156),\n",
       " ('eyes', 153),\n",
       " ('ships', 152),\n",
       " ('sort', 151),\n",
       " ('sir', 151),\n",
       " ('cried', 149),\n",
       " ('back', 148),\n",
       " ('thought', 147),\n",
       " ('world', 147),\n",
       " ('part', 147),\n",
       " ('know', 145),\n",
       " ('oh', 137),\n",
       " ('whole', 137),\n",
       " ('right', 136),\n",
       " ('aye', 135),\n",
       " ('life', 134),\n",
       " ('crew', 133),\n",
       " ('air', 133),\n",
       " ('thus', 132),\n",
       " ('tell', 128),\n",
       " ('night', 128),\n",
       " ('soon', 128),\n",
       " ('thee', 128),\n",
       " ('take', 126),\n",
       " ('came', 126),\n",
       " ('things', 125),\n",
       " ('hands', 125),\n",
       " ('feet', 123),\n",
       " ('small', 122),\n",
       " ('pequod', 121),\n",
       " ('till', 119),\n",
       " ('something', 118),\n",
       " ('think', 116),\n",
       " ('line', 116),\n",
       " ('god', 115),\n",
       " ('towards', 113),\n",
       " ('thy', 113),\n",
       " ('found', 113),\n",
       " ('full', 111),\n",
       " ('another', 110),\n",
       " ('dont', 110),\n",
       " ('along', 110),\n",
       " ('times', 110),\n",
       " ('make', 109),\n",
       " ('nothing', 109),\n",
       " ('whaling', 108)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occurrences = kvwords.groupByKey()\n",
    "occurrences = occurrences.map(lambda x : (x[0], len(x[1])))\n",
    "occurrences = occurrences.sortBy(lambda x: x[1], ascending=False)\n",
    "occurrences.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Actions on Pair RDDs\n",
    "\n",
    "<p justify=\"align\">\n",
    "<font size=\"3\">\n",
    "As with the transformations, all actions available for standard RDDs can be used on Pair RDDs as well.\n",
    "In addition, the following actions can be performed on Pair RDDs:\n",
    "<ol>\n",
    "<li> $countByKey()$. Counts the number of values associated with the same key. Returns a dictionary.\n",
    "<li> $collecAsMap()$. Collects the RDD as a dictionary (in the same way as the function $collect()$ returns a list from a standard RDD).\n",
    "<li> $lookup(key)$. Returns a list with all the values associated with the given _key_.\n",
    "</ol>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p justify=\"align\">\n",
    "<font size=\"3\">\n",
    "Referring again to the example of counting the occurrences of each word, the following code is yet another way to solve the problem by using the action $countByKey()$ on the RDD $kvwords$.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "<font size=\"3\" color='#91053d'>**Execute the following code**</font>\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-100 most frequent words\n",
      "whale ,  891\n",
      "one ,  875\n",
      "old ,  436\n",
      "man ,  433\n",
      "ahab ,  417\n",
      "ye ,  416\n",
      "ship ,  378\n",
      "whales ,  363\n",
      "sea ,  363\n",
      "though ,  362\n",
      "time ,  321\n",
      "long ,  313\n",
      "captain ,  292\n",
      "great ,  289\n",
      "said ,  288\n",
      "seemed ,  280\n",
      "must ,  279\n",
      "two ,  278\n",
      "last ,  273\n",
      "head ,  263\n",
      "see ,  257\n",
      "way ,  253\n",
      "white ,  247\n",
      "little ,  247\n",
      "boat ,  241\n",
      "round ,  239\n",
      "three ,  235\n",
      "sperm ,  232\n",
      "first ,  229\n",
      "stubb ,  227\n",
      "men ,  224\n",
      "say ,  223\n",
      "every ,  223\n",
      "us ,  221\n",
      "well ,  221\n",
      "much ,  218\n",
      "queequeg ,  211\n",
      "good ,  195\n",
      "hand ,  194\n",
      "side ,  183\n",
      "go ,  183\n",
      "thing ,  181\n",
      "look ,  178\n",
      "boats ,  175\n",
      "made ,  174\n",
      "away ,  173\n",
      "chapter ,  172\n",
      "come ,  170\n",
      "starbuck ,  169\n",
      "many ,  161\n",
      "deck ,  158\n",
      "water ,  158\n",
      "far ,  157\n",
      "seen ,  156\n",
      "day ,  156\n",
      "eyes ,  153\n",
      "ships ,  152\n",
      "sir ,  151\n",
      "sort ,  151\n",
      "cried ,  149\n",
      "back ,  148\n",
      "world ,  147\n",
      "part ,  147\n",
      "thought ,  147\n",
      "know ,  145\n",
      "whole ,  137\n",
      "oh ,  137\n",
      "right ,  136\n",
      "aye ,  135\n",
      "life ,  134\n",
      "crew ,  133\n",
      "air ,  133\n",
      "thus ,  132\n",
      "thee ,  128\n",
      "night ,  128\n",
      "tell ,  128\n",
      "soon ,  128\n",
      "came ,  126\n",
      "take ,  126\n",
      "things ,  125\n",
      "hands ,  125\n",
      "feet ,  123\n",
      "small ,  122\n",
      "pequod ,  121\n",
      "till ,  119\n",
      "something ,  118\n",
      "line ,  116\n",
      "think ,  116\n",
      "god ,  115\n",
      "thy ,  113\n",
      "found ,  113\n",
      "towards ,  113\n",
      "full ,  111\n",
      "times ,  110\n",
      "along ,  110\n",
      "dont ,  110\n",
      "another ,  110\n",
      "make ,  109\n",
      "nothing ,  109\n",
      "called ,  108\n"
     ]
    }
   ],
   "source": [
    "occurrences = sorted(kvwords.countByKey().items(), key=operator.itemgetter(1))\n",
    "# Reverse the order\n",
    "occurrences.reverse()\n",
    "print(\"Top-100 most frequent words\")\n",
    "i = 0\n",
    "for (w, f) in occurrences:\n",
    "    i += 1\n",
    "    if i > 100:\n",
    "        break\n",
    "    print(w, \", \", f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Exercises\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Now it's your turn to apply the notions that you've learned so far to create some more complex applications.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    "##  Exercise 3\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In the folder _./data/bbc_ you'll find a collection of 50 documents from the BBC news website corresponding to stories in five topics from 2004-2005. The five topics are: _business_, _entertainment_, _politics_, _sport_ and _tech_. \n",
    "In the directory, the stories are text files (named _001.txt_, _002.txt_...) organized into five directories, one for topic.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In this exercise, we want to create an **inverted index**, one that associates each word to the list of files the word occurs in.\n",
    "More precisely, for each word, the inverted index will have a list of the names  of the files (path relative to the folder _./data_) that contain the word. The figure below shows the entry in the index for the word \"family\".\n",
    "\n",
    "<img src=\"./figs/inverted-index.png\" width=400>\n",
    "    An inverted index is an essential component of a search engine. In fact, given any word, the inverted index allows the search engine to quickly retrieve all documents containing that word.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The idea is to use a **MapReduce** schema. \n",
    "The **Map** task will create a key-value pair $(w, d)$ for each word $w$ that appears in the document $d$. $d$ is the path of the document relative to the folder _./data_ (e.g., _./data/bbc/business/001.txt_).\n",
    "The **Reduce** task will group by key all the key-value pairs with the same key to form the inverted index.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\" color='#91053d'>**Write a Spark application to create an inverted index for the BBC collection. By using the function $lookup$ (already provided), print the documents that contain the word 'family'.**\n",
    "<br>\n",
    "HINT:\n",
    "<ol>\n",
    "<li> Your program needs to go through all the documents in each of the five folders of the BBC collection. Click <a href=\"https://stackoverflow.com/questions/10377998/how-can-i-iterate-over-files-in-a-given-directory\">here</a> to find out how to loop over the files of a directory in Python.\n",
    "<li> To preprocess the text of a document, you can use the function $preprocess()$ defined above. The RDD $words$ returned by this function contains a word as many times as it appears in a document. Since for an inverted index we just need to know that a word is in a specific document, you should remove the duplicates from the RDD $words$.\n",
    "</ol>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents containing 'family':\n",
      "bbc/entertainment/002.txt\n",
      "bbc/entertainment/005.txt\n",
      "bbc/entertainment/003.txt\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "def create_inverted_index():\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"InvertedIndex\") \\\n",
    "        .master(\"local\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Get SparkContext from SparkSession\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    # Base directory for BBC data\n",
    "    base_dir = \"./data/bbc\"\n",
    "    \n",
    "    # Initialize empty RDD to store all (word, document) pairs\n",
    "    word_doc_pairs = sc.emptyRDD()\n",
    "    \n",
    "    # Iterate through all topic directories\n",
    "    for topic in ['business', 'entertainment', 'politics', 'sport', 'tech']:\n",
    "        topic_dir = os.path.join(base_dir, topic)\n",
    "        \n",
    "        # Process each document in the topic directory\n",
    "        for filename in os.listdir(topic_dir):\n",
    "            if filename.endswith('.txt'):\n",
    "                # Get full file path\n",
    "                file_path = os.path.join(topic_dir, filename)\n",
    "                relative_path = os.path.join('bbc', topic, filename)\n",
    "                \n",
    "                # Read and preprocess document\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                    \n",
    "                # Create RDD from text and preprocess\n",
    "                words = sc.parallelize(text.split()).map(lambda x: x.lower())\n",
    "                \n",
    "                # Remove duplicates (we only need to know if a word appears in a document)\n",
    "                words = words.distinct()\n",
    "                \n",
    "                # Create (word, document) pairs\n",
    "                doc_pairs = words.map(lambda word: (word, relative_path))\n",
    "                \n",
    "                # Union with existing pairs\n",
    "                word_doc_pairs = word_doc_pairs.union(doc_pairs)\n",
    "    \n",
    "    # Group by word to create inverted index\n",
    "    inverted_index = word_doc_pairs.groupByKey().mapValues(list)\n",
    "    \n",
    "    return inverted_index\n",
    "\n",
    "def lookup(index, word):\n",
    "    \"\"\"\n",
    "    Returns the list of documents containing the word\n",
    "    \"\"\"\n",
    "    result = index.filter(lambda x: x[0] == word).collect()\n",
    "    if result:\n",
    "        return result[0][1]\n",
    "    return []\n",
    "\n",
    "# Create inverted index\n",
    "index = create_inverted_index()\n",
    "\n",
    "# Look up documents containing 'family'\n",
    "family_docs = lookup(index, 'family')\n",
    "print(\"Documents containing 'family':\")\n",
    "for doc in family_docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !grep -i 'family' ./data/bbc/*/*.txt\n",
    "\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    "##  Exercise 4\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Given the BBC collection, we want to calculate the **co-occurrence matrix** $M$, such that $M[w_1][w_2]$ is the number of documents in which two words $w_1$ and $w_2$ appear together.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "A co-occurrence matrix is generally sparse (i.e., lot of entries are zero), because we expect the pairs of words that have no co-occurrence to be far more than the ones that have at least one co-occurrence. It is certainly wise to represent only the non-zero elements of the matrix.\n",
    "To this extent, we use a **MapReduce** schema.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "For any two words $w_i$, $w_j$ that co-occur in the same document, the **Map** task will output a key-value pair $((w_i, w_j), 1)$, where the key is the pair of co-occurring words $(w_i, w_j)$ and the value is 1.\n",
    "Note that the two key-value pairs $((w_i, w_j), 1)$ and $((w_j, w_i), 1)$ ought to be considered as having the same key, because the order of occurrence of the two words does not matter (the only thing that matters is whether the two words co-occur in the same document). \n",
    "For this reason, the key-value pair $((w_i, w_j), 1)$ returned by the Map task is such that $w_i < w_j$, where $<$ is the lexicographic order.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In the output of the Map task, a key-value pair $((w_i, w_j), 1)$ appears as many times as documents where $w_i$ and $w_j$ co-occur. In order to obtain the number of co-occurrences of $w_i$ and $w_j$, the **Reduce** task will just have to sum the values associated to the key $(w_i, w_j)$.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\" color='#91053d'>**Write a Spark application that creates the co-occurrence matrix of the documents in the BBC collection. Print the top-20 most frequent co-occurrences.**\n",
    "<br>\n",
    "HINT: as before, use the function $preprocess()$ and remove the duplicate words.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most frequent co-occurrences:\n",
      "(and, the): 50\n",
      "(a, and): 49\n",
      "(in, the): 49\n",
      "(the, to): 49\n",
      "(and, in): 49\n",
      "(and, to): 49\n",
      "(a, the): 49\n",
      "(a, in): 48\n",
      "(in, to): 48\n",
      "(a, to): 48\n",
      "(of, the): 47\n",
      "(is, the): 47\n",
      "(and, is): 47\n",
      "(of, to): 47\n",
      "(and, of): 47\n",
      "(is, to): 46\n",
      "(a, is): 46\n",
      "(and, for): 46\n",
      "(a, of): 46\n",
      "(in, of): 46\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from itertools import combinations\n",
    "\n",
    "def create_cooccurrence_matrix():\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"CooccurrenceMatrix\") \\\n",
    "        .master(\"local\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    # Base directory for BBC data\n",
    "    base_dir = \"./data/bbc\"\n",
    "    \n",
    "    # Initialize empty RDD for word pairs\n",
    "    word_pairs = sc.emptyRDD()\n",
    "    \n",
    "    # Process each topic directory\n",
    "    for topic in ['business', 'entertainment', 'politics', 'sport', 'tech']:\n",
    "        topic_dir = os.path.join(base_dir, topic)\n",
    "        \n",
    "        # Process each document\n",
    "        for filename in os.listdir(topic_dir):\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(topic_dir, filename)\n",
    "                \n",
    "                # Read and preprocess document\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                \n",
    "                # Create RDD from text and preprocess\n",
    "                words = sc.parallelize(text.split()).map(lambda x: x.lower())\n",
    "                \n",
    "                # Remove duplicates within document\n",
    "                words = words.distinct()\n",
    "                \n",
    "                # Generate word pairs (ensuring w1 < w2 lexicographically)\n",
    "                doc_pairs = words.collect()\n",
    "                pairs = [(w1, w2) for w1, w2 in combinations(sorted(doc_pairs), 2)]\n",
    "                \n",
    "                # Create RDD of pairs with count 1\n",
    "                doc_word_pairs = sc.parallelize(pairs).map(lambda x: (x, 1))\n",
    "                \n",
    "                # Union with existing pairs\n",
    "                word_pairs = word_pairs.union(doc_word_pairs)\n",
    "    \n",
    "    # Reduce by key to get co-occurrence counts\n",
    "    cooccurrence_matrix = word_pairs.reduceByKey(lambda x, y: x + y)\n",
    "    \n",
    "    # Sort by count in descending order and take top 20\n",
    "    top_cooccurrences = cooccurrence_matrix.sortBy(lambda x: x[1], ascending=False).take(20)\n",
    "    \n",
    "    return top_cooccurrences\n",
    "\n",
    "# Create co-occurrence matrix and get top 20\n",
    "top_20 = create_cooccurrence_matrix()\n",
    "\n",
    "# Print results\n",
    "print(\"Top 20 most frequent co-occurrences:\")\n",
    "for (w1, w2), count in top_20:\n",
    "    print(f\"({w1}, {w2}): {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    "##  Exercise 5\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "One important task in the analysis of textual data is the computation of the similarity between two textual documents.\n",
    "Intuitively, the more words two textual documents share, the more similar they are.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Let $d_1$ and $d_2$ be two documents, and let $W(d_1)$ and $W(d_2)$ be the set of words in $d_1$ and $d_2$, respectively. The similarity $S(d_1, d_2)$ between $d_1$ and $d_2$ can be computed with the _Jaccard_ score as follows:\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "</p>\n",
    "\n",
    "$$S(d_1, d_2) = \\frac{W(d_1) \\cap W(d_2)}{W(d_1) \\cup W(d_2)}$$\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\" color='#91053d'>**Write a function $jaccard$ that takes in two RDDs containing the words of two documents and returns the Jaccard similarity score of the documents. Test the function on the documents of the BBC.**\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard similarity between ./data/bbc/business/001.txt and ./data/bbc/sport/001.txt: 0.0852\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text by removing non-alphabetic characters and converting to lowercase.\n",
    "    :param text: Raw text string\n",
    "    :return: List of cleaned words\n",
    "    \"\"\"\n",
    "    regex = re.compile('[^a-zA-Z ]')  # Keep only letters and spaces\n",
    "    text = regex.sub('', text)  # Remove non-alphabetic characters\n",
    "    words = text.lower().split()  # Convert to lowercase and split into words\n",
    "    return words\n",
    "\n",
    "def jaccard(rdd1, rdd2):\n",
    "    \"\"\"\n",
    "    Computes the Jaccard similarity score between two documents.\n",
    "    :param rdd1: RDD containing the words of document 1\n",
    "    :param rdd2: RDD containing the words of document 2\n",
    "    :return: Jaccard similarity score\n",
    "    \"\"\"\n",
    "    # Remove duplicates to get unique words in each document\n",
    "    words1 = rdd1.distinct()\n",
    "    words2 = rdd2.distinct()\n",
    "\n",
    "    # Calculate intersection (common words)\n",
    "    intersection = words1.intersection(words2).count()\n",
    "\n",
    "    # Calculate union\n",
    "    union = words1.union(words2).distinct().count()\n",
    "\n",
    "    # Return Jaccard similarity score\n",
    "    return float(intersection) / union if union > 0 else 0.0\n",
    "\n",
    "def read_document(sc, filepath):\n",
    "    \"\"\"\n",
    "    Reads a document and converts it into an RDD of words after preprocessing.\n",
    "    :param sc: SparkContext\n",
    "    :param filepath: Path to the document\n",
    "    :return: RDD containing the words of the document\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    words = preprocess(text)  # Preprocess the text into words\n",
    "    return sc.parallelize(words)  # Create an RDD from the list of words\n",
    "\n",
    "def test_jaccard_similarity():\n",
    "    \"\"\"\n",
    "    Tests the Jaccard similarity function using two BBC documents.\n",
    "    \"\"\"\n",
    "    # Initialize Spark Session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"JaccardSimilarity\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Base directory for the BBC dataset\n",
    "    base_dir = \"./data/bbc\"\n",
    "    \n",
    "    # Paths to two example documents\n",
    "    doc1_path = os.path.join(base_dir, \"business/001.txt\")\n",
    "    doc2_path = os.path.join(base_dir, \"sport/001.txt\")\n",
    "    \n",
    "    # Read and preprocess the documents\n",
    "    rdd1 = read_document(sc, doc1_path)\n",
    "    rdd2 = read_document(sc, doc2_path)\n",
    "    \n",
    "    # Compute the Jaccard similarity\n",
    "    similarity = jaccard(rdd1, rdd2)\n",
    "    print(f\"Jaccard similarity between {doc1_path} and {doc2_path}: {similarity:.4f}\")\n",
    "    \n",
    "    # Stop SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_jaccard_similarity()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    "##  Exercise 6\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We are now going to use the similarity function that we defined in Exercise 4 to create a simple text classifier.\n",
    "A _text classifier_ is an application that takes in a textual document and determines the topic that the document covers.\n",
    "Usually, text classification is based on supervised machine learning algorithms, like Naive Bayes and SVM; \n",
    "since these algorithms are out of the scope of this course, we limit ourselves to a simple classifier that assigns a document to the topic with the highest similarity to the document itself.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "More precisely, let's consider five documents $d_1, \\ldots, d_5$, one for each topic of the BBC dataset; arbitrarily, we say that each of these documents is representative of one topic. \n",
    "Now, consider a document $d$ that is different from the representative documents. \n",
    "The text classifier computes the similarity of $d$ to $d_1, \\ldots, d_5$, selects the document $d_i$ that has the highest similarity to $d$ and assigns $d$ to the topic covered by $d_i$.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\" color='#91053d'>**Write a function $classify$ that takes in a document $d$ (already preprocessed) and the list of representative documents (already preprocessed) for each topic and outputs the predicted topic of $d$.**\n",
    "</font>\n",
    "</p>\n",
    "<p>\n",
    "<font size=\"3\" color='#91053d'>\n",
    "HINT: You can use the files named _001.txt_ as the representative documents for each topic.   \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity with business: 0.1167\n",
      "Similarity with entertainment: 0.0882\n",
      "Similarity with politics: 0.1129\n",
      "Similarity with sport: 0.1209\n",
      "Similarity with tech: 0.0902\n",
      "Predicted topic: sport\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text by removing non-alphabetic characters and converting to lowercase.\n",
    "    :param text: Raw text string\n",
    "    :return: List of cleaned words\n",
    "    \"\"\"\n",
    "    regex = re.compile('[^a-zA-Z ]')  # Keep only letters and spaces\n",
    "    text = regex.sub('', text)  # Remove non-alphabetic characters\n",
    "    words = text.lower().split()  # Convert to lowercase and split into words\n",
    "    return words\n",
    "\n",
    "def jaccard(rdd1, rdd2):\n",
    "    \"\"\"\n",
    "    Computes the Jaccard similarity score between two documents.\n",
    "    :param rdd1: RDD containing the words of document 1\n",
    "    :param rdd2: RDD containing the words of document 2\n",
    "    :return: Jaccard similarity score\n",
    "    \"\"\"\n",
    "    # Remove duplicates to get unique words in each document\n",
    "    words1 = rdd1.distinct()\n",
    "    words2 = rdd2.distinct()\n",
    "\n",
    "    # Calculate intersection (common words)\n",
    "    intersection = words1.intersection(words2).count()\n",
    "\n",
    "    # Calculate union\n",
    "    union = words1.union(words2).distinct().count()\n",
    "\n",
    "    # Return Jaccard similarity score\n",
    "    return float(intersection) / union if union > 0 else 0.0\n",
    "\n",
    "def read_document(sc, filepath):\n",
    "    \"\"\"\n",
    "    Reads a document and converts it into an RDD of words after preprocessing.\n",
    "    :param sc: SparkContext\n",
    "    :param filepath: Path to the document\n",
    "    :return: RDD containing the words of the document\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    words = preprocess(text)  # Preprocess the text into words\n",
    "    return sc.parallelize(words)  # Create an RDD from the list of words\n",
    "\n",
    "def classify(document_rdd, representative_docs):\n",
    "    \"\"\"\n",
    "    Classifies a document into one of the topics based on similarity to representative documents.\n",
    "    :param document_rdd: RDD containing the words of the document to classify\n",
    "    :param representative_docs: Dictionary where keys are topics and values are RDDs of representative documents\n",
    "    :return: Predicted topic of the document\n",
    "    \"\"\"\n",
    "    max_similarity = -1  # Initialize the maximum similarity score\n",
    "    predicted_topic = None  # Initialize the predicted topic\n",
    "\n",
    "    # Compare the document with each representative document\n",
    "    for topic, rep_doc_rdd in representative_docs.items():\n",
    "        # Compute similarity with the representative document\n",
    "        similarity = jaccard(document_rdd, rep_doc_rdd)\n",
    "        print(f\"Similarity with {topic}: {similarity:.4f}\")  # Debugging\n",
    "\n",
    "        # Update the predicted topic if the similarity is higher\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            predicted_topic = topic\n",
    "\n",
    "    return predicted_topic\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"TextClassifier\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Base directory for the BBC dataset\n",
    "    base_dir = \"./data/bbc\"\n",
    "\n",
    "    # Load representative documents\n",
    "    topics = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "    representative_docs = {}\n",
    "    for topic in topics:\n",
    "        rep_doc_path = os.path.join(base_dir, topic, \"001.txt\")\n",
    "        representative_docs[topic] = read_document(sc, rep_doc_path)\n",
    "\n",
    "    # Load a test document to classify\n",
    "    test_doc_path = os.path.join(base_dir, \"business/002.txt\")  # Replace with any test document\n",
    "    test_doc_rdd = read_document(sc, test_doc_path)\n",
    "\n",
    "    # Classify the test document\n",
    "    predicted_topic = classify(test_doc_rdd, representative_docs)\n",
    "    print(f\"Predicted topic: {predicted_topic}\")\n",
    "\n",
    "    # Stop SparkSession\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    "##  Exercise 7 \n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "\n",
    "After implementing a classifier, we need to evaluate it. The evaluation is based on the **confusion matrix** that describes the performance of the classifier on a test dataset for which the true values are known.\n",
    "An example of confusion matrix is shown in the figure below. \n",
    "Each row contains the instances in a predicted class while each column represents the instances in an actual class.\n",
    "From the matrix in the Figure, we learn that there are 10 documents that are in the class \"business\" (see the total of the first column), of which 5 are correctly predicted as being in the class \"business\", 2 are incorrectly classified in the class \"entertainment\" and  3 are incorrectly classified in the class \"sport\".\n",
    "</font>\n",
    "<img src=\"./figs/confusion-matrix.png\" width=800>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "For each class $C$, we can define the notion of true positives (TP), false positives (FP) and false negatives (FN) for that class (the figure shows the TP, FP and FN for the class \"Business\"):\n",
    "<ul>\n",
    "<li> **True positives** (TP). Set of documents whose actual and predicted class is $C$.\n",
    "<li> **False positives** (FP). Set of documents whose predicted class is $C$, while the actual class is not $C$.\n",
    "<li> **False negatives** (FN). Set of documents whose predicted class is not $C$, while the actual class is $C$.\n",
    "</ul>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "For each class $C$, we can compute three evaluation measures, called _precision_ ($P_C$), _recall_ ($R_C$) and \n",
    "_f-measure_ ($F_C$), as follows:\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "$$P_C = \\frac{|TP|}{|TP| + |FP|}\\quad R_C = \\frac{|TP|}{|TP| + |FN|}\\quad F_C = 2\\cdot \\frac{P_C\\cdot R_C}{P_C + R_C}$$\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The _precision_ $P_C$ tells how many of the documents classified in $C$ are correctly classified;\n",
    "the _recall_ $R_C$ tells how many of the documents that are actually in $C$ are correctly classified in $C$;\n",
    "the _f-measure_ is an harmonic mean of _precision_ and _recall_ and gives an indication of the overall _accuracy_ of the classifier.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\" color='#91053d'>**Write a program that evaluates the classifier implemented in the previous exercise on the whole BBC dataset. Print precision, recall and f-measure for each class.\n",
    "Compare different classifiers using different representative documents for the topics**\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "               business        entertainment   politics        sport           tech           \n",
      "business       5               0               2               2               0              \n",
      "entertainment  1               5               1               1               1              \n",
      "politics       0               0               8               1               0              \n",
      "sport          0               0               0               9               0              \n",
      "tech           0               1               4               1               3              \n",
      "\n",
      "Evaluation Metrics:\n",
      "Business:\n",
      "  Precision: 0.8333\n",
      "  Recall:    0.5556\n",
      "  F-Measure: 0.6667\n",
      "Entertainment:\n",
      "  Precision: 0.8333\n",
      "  Recall:    0.5556\n",
      "  F-Measure: 0.6667\n",
      "Politics:\n",
      "  Precision: 0.5333\n",
      "  Recall:    0.8889\n",
      "  F-Measure: 0.6667\n",
      "Sport:\n",
      "  Precision: 0.6429\n",
      "  Recall:    1.0000\n",
      "  F-Measure: 0.7826\n",
      "Tech:\n",
      "  Precision: 0.7500\n",
      "  Recall:    0.3333\n",
      "  F-Measure: 0.4615\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "import builtins\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text by removing non-alphabetic characters and converting to lowercase.\n",
    "    \"\"\"\n",
    "    regex = re.compile('[^a-zA-Z ]')  # Only keep letters and spaces\n",
    "    text = regex.sub('', text)  # Remove non-alphabetic characters\n",
    "    words = text.lower().split()  # Convert to lowercase and split into words\n",
    "    return words\n",
    "\n",
    "\n",
    "def jaccard(rdd1, rdd2):\n",
    "    \"\"\"\n",
    "    Computes the Jaccard similarity score between two documents.\n",
    "    \"\"\"\n",
    "    words1 = rdd1.distinct()\n",
    "    words2 = rdd2.distinct()\n",
    "\n",
    "    intersection = words1.intersection(words2).count()\n",
    "    union = words1.union(words2).distinct().count()\n",
    "\n",
    "    return float(intersection) / union if union > 0 else 0.0\n",
    "\n",
    "\n",
    "def read_document(sc, filepath):\n",
    "    \"\"\"\n",
    "    Reads a document and converts it into an RDD of words after preprocessing.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    words = preprocess(text)\n",
    "    return sc.parallelize(words)\n",
    "\n",
    "\n",
    "def classify(doc_rdd, representative_docs):\n",
    "    \"\"\"\n",
    "    Classifies a document based on its similarity to representative documents.\n",
    "    \"\"\"\n",
    "    # Compute similarity for each topic\n",
    "    similarities = {\n",
    "        topic: jaccard(doc_rdd, rep_doc_rdd)\n",
    "        for topic, rep_doc_rdd in representative_docs.items()\n",
    "    }\n",
    "\n",
    "    # Return the topic with the highest similarity\n",
    "    predicted_topic = max(similarities.items(), key=lambda x: x[1])[0]\n",
    "    return predicted_topic\n",
    "\n",
    "\n",
    "def evaluate_classifier(sc, base_dir, representative_docs):\n",
    "    \"\"\"\n",
    "    Evaluates the classifier on the whole BBC dataset.\n",
    "    Computes precision, recall, and f-measure for each topic.\n",
    "    \"\"\"\n",
    "    topics = list(representative_docs.keys())\n",
    "    confusion_matrix = {topic: {t: 0 for t in topics} for topic in topics}\n",
    "\n",
    "    # Classify all documents except the representative ones\n",
    "    for topic in topics:\n",
    "        topic_dir = os.path.join(base_dir, topic)\n",
    "        for filename in os.listdir(topic_dir):\n",
    "            if filename.endswith('.txt') and filename != \"001.txt\":  # Skip representative document\n",
    "                doc_path = os.path.join(topic_dir, filename)\n",
    "                doc_rdd = read_document(sc, doc_path)\n",
    "                predicted_topic = classify(doc_rdd, representative_docs)\n",
    "\n",
    "                # Update confusion matrix\n",
    "                confusion_matrix[topic][predicted_topic] += 1\n",
    "\n",
    "    metrics = {}\n",
    "    for topic in topics:\n",
    "        tp = confusion_matrix[topic][topic]\n",
    "        fp = builtins.sum(confusion_matrix[t][topic] for t in topics if t != topic)\n",
    "        fn = builtins.sum(confusion_matrix[topic][t] for t in topics if t != topic)\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f_measure = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        metrics[topic] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f_measure\": f_measure\n",
    "        }\n",
    "\n",
    "    return confusion_matrix, metrics\n",
    "\n",
    "\n",
    "def test_classifier_with_evaluation():\n",
    "    \"\"\"\n",
    "    Tests the classifier and evaluates it on the BBC dataset.\n",
    "    \"\"\"\n",
    "    # Initialize Spark\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"TextClassifierEvaluation\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Base directory for the BBC dataset\n",
    "    base_dir = \"./data/bbc\"\n",
    "    topics = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "\n",
    "    # Load representative documents (use 001.txt as default)\n",
    "    representative_docs = {}\n",
    "    for topic in topics:\n",
    "        rep_doc_path = os.path.join(base_dir, topic, \"001.txt\")\n",
    "        if os.path.exists(rep_doc_path):\n",
    "            rep_doc_rdd = read_document(sc, rep_doc_path)\n",
    "            representative_docs[topic] = rep_doc_rdd\n",
    "        else:\n",
    "            print(f\"Warning: Representative document not found for topic {topic}\")\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    confusion_matrix, metrics = evaluate_classifier(sc, base_dir, representative_docs)\n",
    "\n",
    "    # Print confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"{'':<15}\" + \" \".join(f\"{t:<15}\" for t in topics))\n",
    "    for actual_topic, row in confusion_matrix.items():\n",
    "        print(f\"{actual_topic:<15}\" + \" \".join(f\"{row[predicted_topic]:<15}\" for predicted_topic in topics))\n",
    "\n",
    "    # Print precision, recall, and f-measure for each topic\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    for topic, metric in metrics.items():\n",
    "        print(f\"{topic.capitalize()}:\")\n",
    "        print(f\"  Precision: {metric['precision']:.4f}\")\n",
    "        print(f\"  Recall:    {metric['recall']:.4f}\")\n",
    "        print(f\"  F-Measure: {metric['f_measure']:.4f}\")\n",
    "\n",
    "    # Stop Spark\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_classifier_with_evaluation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "name": "BE4-Spark.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
